{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as scipy\n",
    "import os\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import concise.layers as cl\n",
    "from keras.models import Model, load_model\n",
    "import keras.layers as kl\n",
    "from keras import regularizers\n",
    "import keras.optimizers as ko\n",
    "from keras.callbacks import EarlyStopping\n",
    "from pylab import find\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "from concise.preprocessing import encodeDNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../helper')\n",
    "import common as cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Create model based on training dataset\n",
    "- Evaluate model using test dataset\n",
    "- Use fit_generator library of Keras to fit model with large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_sm( train_data,\n",
    "              filters=64, # use powers of 2 - (32, 64, 128)\n",
    "              motif_width=4, # use odd numbers - 11 or 15\n",
    "              use_1by1_conv=False, # hp.choice(True, False(\n",
    "              # regularization\n",
    "              l1=1e-06, l1_1 = 1e-08, l1_2 = 1e-9, l1_3 = 1e-10,  \n",
    "              use_two_random_seq = True,\n",
    "              first_layer_units = 128, # (64, 128, 256, 512)\n",
    "              lr=0.001, useIndex = False, noDense = False):\n",
    "    inputs = []\n",
    "    convolute = cl.ConvDNA(filters=filters, \n",
    "                   kernel_size=motif_width, activation=\"relu\")\n",
    "    convolute1by1 = kl.Conv1D(filters=filters,\n",
    "                       kernel_size=1, activation='relu')\n",
    "    \n",
    "    dense = kl.Dense(units=1,\n",
    "                     kernel_regularizer=regularizers.l1(l1))\n",
    "    \n",
    "    dense1 = kl.Dense(units=1,\n",
    "                     kernel_regularizer=regularizers.l1(l1_1))\n",
    "        \n",
    "    dense2 = kl.Dense(units=1,\n",
    "                     kernel_regularizer=regularizers.l1(l1_2))\n",
    "    \n",
    "    dense3 = kl.Dense(units=1,\n",
    "                     kernel_regularizer=regularizers.l1(l1_3))\n",
    "    \n",
    "    inputs = [kl.Input(shape=(train_data[0]['seq'].shape[2],train_data[0]['seq'].shape[3]),\n",
    "                      name='seq' + str(i)) for i in range(train_data[0]['seq'].shape[1])]\n",
    "    \n",
    "\n",
    "    #b = kl.Lambda(slice)(nw_inputs)\n",
    "    \n",
    "    #print(b)\n",
    "    x = []\n",
    "    for i in range(train_data[0]['seq'].shape[1]):\n",
    "        input_dna = inputs[i]\n",
    "        xi = convolute(input_dna)\n",
    "        xi = convolute1by1(xi)\n",
    "        xi = kl.Flatten()(xi)\n",
    "        xi = dense(xi)\n",
    "        \n",
    "        if i==0 or i==28 or i == 56:\n",
    "            xi = kl.Dense(units=1,\n",
    "                     kernel_regularizer=regularizers.l1(l1_1))(xi)\n",
    "        elif i < 28:\n",
    "            xi = kl.Dense(units=1,\n",
    "                     kernel_regularizer=regularizers.l1(l1_2))(xi)\n",
    "        else:\n",
    "             xi = kl.Dense(units=1,\n",
    "                     kernel_regularizer=regularizers.l1(l1_3))(xi)\n",
    "            \n",
    "\n",
    "        #xi = kl.Dense(units=1)(xi) \n",
    "        #xi = kl.Activation('sigmoid')(xi) \n",
    "        \n",
    "        x = x + [xi]\n",
    "    \n",
    "    #print(x)\n",
    "    x = kl.Concatenate(axis = 1)(x)\n",
    "    x = kl.Activation(\"softmax\")(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    \n",
    "    model.compile(optimizer=ko.Adam(lr=lr), loss=\"kullback_leibler_divergence\", metrics=[cm.r2_score_k])\n",
    "    #print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model_fit_generator(model, train_data_file, val_data_file, batch_size=256):\n",
    "    \"\"\" Train the video classification model\n",
    "    \"\"\"\n",
    "    with h5py.File(train_data_file, \"r\") as train_data, h5py.File(val_data_file, \"r\") as val_data:\n",
    "        sample_count = int(train_data.attrs[\"sample_count\"])\n",
    "        sample_idxs = range(0, sample_count)\n",
    "        training_sample_idxs = np.random.permutation(sample_idxs)\n",
    " \n",
    "        sample_count = int(val_data.attrs[\"sample_count\"])\n",
    "        sample_idxs = range(0, sample_count)\n",
    "        validation_sample_idxs = np.random.permutation(sample_idxs)\n",
    "        \n",
    "        training_sequence_generator = generate_training_sequences(batch_size,\n",
    "                                                                   train_data,\n",
    "                                                                   training_sample_idxs)\n",
    "        validation_sequence_generator = generate_validation_sequences(batch_size,\n",
    "                                                                       val_data,\n",
    "                                                                       validation_sample_idxs)\n",
    "        validation_steps=int(len(validation_sample_idxs)/batch_size)\n",
    "        #model.fit_generator(generator=training_sequence_generator,\n",
    "        #                     validation_data=validation_sequence_generator,\n",
    "        #                     samples_per_epoch=len(training_sample_idxs),\n",
    "        #                     nb_val_samples=len(validation_sample_idxs),\n",
    "        #                     nb_epoch=100,\n",
    "        #                     max_q_size=1,\n",
    "        #                     verbose=2,\n",
    "        #                     callbacks=[EarlyStopping(patience=5)],\n",
    "        #                     class_weight=None,\n",
    "        #                     nb_worker=5)\n",
    "        model.fit_generator(generator=training_sequence_generator,\n",
    "                            steps_per_epoch = int(len(training_sample_idxs)/batch_size),\n",
    "                            epochs=300, \n",
    "                            callbacks=[EarlyStopping(patience=3)],\n",
    "                            validation_data=validation_sequence_generator,\n",
    "                            validation_steps=validation_steps,\n",
    "                            workers=1)    \n",
    "\n",
    "def generate_training_sequences(batch_size, train_data, training_sample_idxs):\n",
    "    \"\"\" Generates training sequences on demand\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # generate sequences for training\n",
    "        training_sample_count = len(training_sample_idxs)\n",
    "        batches = int(training_sample_count/batch_size)\n",
    "        remainder_samples = training_sample_count%batch_size\n",
    "        if remainder_samples:\n",
    "            batches = batches + 1\n",
    "        # generate batches of samples\n",
    "        for idx in range(0, batches):\n",
    "            if idx == batches - 1:\n",
    "                batch_idxs = training_sample_idxs[idx*batch_size:]\n",
    "            else:\n",
    "                batch_idxs = training_sample_idxs[idx*batch_size:idx*batch_size+batch_size]\n",
    "            batch_idxs = sorted(batch_idxs)\n",
    "\n",
    "            X = train_data[\"X\"][batch_idxs]\n",
    "            Y = train_data[\"Y\"][batch_idxs]\n",
    "            \n",
    "            #X, Y = get_data(X, Y)\n",
    "            yield get_data(X, Y)\n",
    "\n",
    "def generate_validation_sequences(batch_size, val_data, validation_sample_idxs):\n",
    "    \"\"\" Generates validation sequences on demand\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        # generate sequences for validation\n",
    "        validation_sample_count = len(validation_sample_idxs)\n",
    "        batches = int(validation_sample_count/batch_size)\n",
    "        remainder_samples = validation_sample_count%batch_size\n",
    "        if remainder_samples:\n",
    "            batches = batches + 1\n",
    "        # generate batches of samples\n",
    "        for idx in range(0, batches):\n",
    "            if idx == batches - 1:\n",
    "                batch_idxs = validation_sample_idxs[idx*batch_size:]\n",
    "            else:\n",
    "                batch_idxs = validation_sample_idxs[idx*batch_size:idx*batch_size+batch_size]\n",
    "            batch_idxs = sorted(batch_idxs)\n",
    "\n",
    "            X = val_data[\"X\"][batch_idxs]\n",
    "            Y = val_data[\"Y\"][batch_idxs]\n",
    "\n",
    "            X, Y = get_data(X, Y)\n",
    "            yield X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    with h5py.File(\"train_data_full.hdf5\", \"r\") as train_data:\n",
    "        XSample = train_data[\"X\"][:2]\n",
    "        YSample = train_data[\"Y\"][:2]\n",
    "\n",
    "    trainSample = get_data(XSample, YSample)\n",
    "    model = get_model_sm(trainSample,             \n",
    "                  filters=128, # use powers of 2 - (32, 64, 128)\n",
    "                  motif_width=15, # use odd numbers - 11 or 15\n",
    "                  use_1by1_conv=True,                     \n",
    "                  l1=1e-08, l1_1 = 1e-08, l1_2 = 1e-10, l1_3 = 1e-12, \n",
    "                  lr=0.0001, noDense = True         \n",
    "\n",
    "    )\n",
    "\n",
    "    train_model_fit_generator(model, \"train_data_full.hdf5\", \"val_data_full.hdf5\")\n",
    "    return model\n",
    "\n",
    "def eval_model(model):\n",
    "    with h5py.File(\"test_data_full.hdf5\", \"r\") as test_data:\n",
    "        XTest = test_data[\"X\"][:]\n",
    "        YTest = test_data[\"Y\"][:]\n",
    "\n",
    "    XTest, YTest =  get_data(XTest, YTest)\n",
    "    ypredict = model.predict(XTest)\n",
    "\n",
    "    print(\"SD1 \" + str(scipy.stats.pearsonr(ypredict[:,0].ravel(), YTest[:,0])[0]**2))\n",
    "    print(\"SD2 \" + str(scipy.stats.pearsonr(ypredict[:,28].ravel(), YTest[:,28])[0]**2))\n",
    "    print(\"SDCrypt \" + str(scipy.stats.pearsonr(ypredict[:,55], YTest[:,55])[0]**2))\n",
    "    print(\"SDNew \" + str(scipy.stats.pearsonr((np.sum(ypredict[:,1:28], axis = 1) + np.sum(ypredict[:,29:54], axis = 1)).ravel()\n",
    "                          , (np.sum(YTest[:,1:28], axis = 1) + np.sum(YTest[:,29:54], axis = 1)).ravel())[0]**2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create folder to save results\n",
    "resultsdir = cm.create_folder(os.path.abspath('../results'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"activation_6_target:0\", shape=(?, ?), dtype=float32)\n",
      "Epoch 1/300\n",
      "585/585 [==============================] - 180s - loss: 1.5894 - r2_score_k: 0.3928 - val_loss: 1.1129 - val_r2_score_k: 0.5176\n",
      "Epoch 2/300\n",
      "585/585 [==============================] - 181s - loss: 1.0819 - r2_score_k: 0.5340 - val_loss: 1.0650 - val_r2_score_k: 0.5463\n",
      "Epoch 3/300\n",
      "585/585 [==============================] - 176s - loss: 1.0463 - r2_score_k: 0.5540 - val_loss: 1.0357 - val_r2_score_k: 0.5612\n",
      "Epoch 4/300\n",
      "585/585 [==============================] - 177s - loss: 1.0172 - r2_score_k: 0.5684 - val_loss: 1.0036 - val_r2_score_k: 0.5777\n",
      "Epoch 5/300\n",
      "585/585 [==============================] - 174s - loss: 0.9829 - r2_score_k: 0.5863 - val_loss: 0.9651 - val_r2_score_k: 0.5976\n",
      "Epoch 6/300\n",
      "585/585 [==============================] - 177s - loss: 0.9435 - r2_score_k: 0.6063 - val_loss: 0.9237 - val_r2_score_k: 0.6168\n",
      "Epoch 7/300\n",
      "585/585 [==============================] - 179s - loss: 0.9066 - r2_score_k: 0.6229 - val_loss: 0.8898 - val_r2_score_k: 0.6310\n",
      "Epoch 8/300\n",
      "585/585 [==============================] - 178s - loss: 0.8731 - r2_score_k: 0.6363 - val_loss: 0.8585 - val_r2_score_k: 0.6422\n",
      "Epoch 9/300\n",
      "585/585 [==============================] - 181s - loss: 0.8435 - r2_score_k: 0.6473 - val_loss: 0.8301 - val_r2_score_k: 0.6521\n",
      "Epoch 10/300\n",
      "585/585 [==============================] - 183s - loss: 0.8161 - r2_score_k: 0.6569 - val_loss: 0.8018 - val_r2_score_k: 0.6621\n",
      "Epoch 11/300\n",
      "585/585 [==============================] - 177s - loss: 0.7904 - r2_score_k: 0.6657 - val_loss: 0.7769 - val_r2_score_k: 0.6694\n",
      "Epoch 12/300\n",
      "585/585 [==============================] - 179s - loss: 0.7657 - r2_score_k: 0.6741 - val_loss: 0.7571 - val_r2_score_k: 0.6736\n",
      "Epoch 13/300\n",
      "585/585 [==============================] - 178s - loss: 0.7426 - r2_score_k: 0.6819 - val_loss: 0.7376 - val_r2_score_k: 0.6781\n",
      "Epoch 14/300\n",
      "585/585 [==============================] - 178s - loss: 0.7203 - r2_score_k: 0.6895 - val_loss: 0.7143 - val_r2_score_k: 0.6867\n",
      "Epoch 15/300\n",
      "585/585 [==============================] - 179s - loss: 0.6990 - r2_score_k: 0.6966 - val_loss: 0.6915 - val_r2_score_k: 0.6954\n",
      "Epoch 16/300\n",
      "585/585 [==============================] - 177s - loss: 0.6785 - r2_score_k: 0.7032 - val_loss: 0.6689 - val_r2_score_k: 0.7061\n",
      "Epoch 17/300\n",
      "585/585 [==============================] - 178s - loss: 0.6586 - r2_score_k: 0.7095 - val_loss: 0.6509 - val_r2_score_k: 0.7120\n",
      "Epoch 18/300\n",
      "585/585 [==============================] - 176s - loss: 0.6396 - r2_score_k: 0.7154 - val_loss: 0.6316 - val_r2_score_k: 0.7176\n",
      "Epoch 19/300\n",
      "585/585 [==============================] - 176s - loss: 0.6212 - r2_score_k: 0.7207 - val_loss: 0.6131 - val_r2_score_k: 0.7228\n",
      "Epoch 20/300\n",
      "585/585 [==============================] - 181s - loss: 0.6037 - r2_score_k: 0.7258 - val_loss: 0.5958 - val_r2_score_k: 0.7267\n",
      "Epoch 21/300\n",
      "585/585 [==============================] - 177s - loss: 0.5861 - r2_score_k: 0.7308 - val_loss: 0.5807 - val_r2_score_k: 0.7307\n",
      "Epoch 22/300\n",
      "585/585 [==============================] - 180s - loss: 0.5694 - r2_score_k: 0.7355 - val_loss: 0.5644 - val_r2_score_k: 0.7349\n",
      "Epoch 23/300\n",
      "585/585 [==============================] - 177s - loss: 0.5528 - r2_score_k: 0.7400 - val_loss: 0.5470 - val_r2_score_k: 0.7403\n",
      "Epoch 24/300\n",
      "585/585 [==============================] - 178s - loss: 0.5365 - r2_score_k: 0.7444 - val_loss: 0.5307 - val_r2_score_k: 0.7446\n",
      "Epoch 25/300\n",
      "585/585 [==============================] - 177s - loss: 0.5200 - r2_score_k: 0.7484 - val_loss: 0.5147 - val_r2_score_k: 0.7484\n",
      "Epoch 26/300\n",
      "585/585 [==============================] - 178s - loss: 0.5023 - r2_score_k: 0.7523 - val_loss: 0.4963 - val_r2_score_k: 0.7524\n",
      "Epoch 27/300\n",
      "585/585 [==============================] - 178s - loss: 0.4818 - r2_score_k: 0.7564 - val_loss: 0.4763 - val_r2_score_k: 0.7558\n",
      "Epoch 28/300\n",
      "585/585 [==============================] - 177s - loss: 0.4593 - r2_score_k: 0.7620 - val_loss: 0.4581 - val_r2_score_k: 0.7596\n",
      "Epoch 29/300\n",
      "585/585 [==============================] - 177s - loss: 0.4385 - r2_score_k: 0.7691 - val_loss: 0.4365 - val_r2_score_k: 0.7684\n",
      "Epoch 30/300\n",
      "585/585 [==============================] - 178s - loss: 0.4206 - r2_score_k: 0.7757 - val_loss: 0.4171 - val_r2_score_k: 0.7759\n",
      "Epoch 31/300\n",
      "585/585 [==============================] - 177s - loss: 0.4048 - r2_score_k: 0.7815 - val_loss: 0.4044 - val_r2_score_k: 0.7789\n",
      "Epoch 32/300\n",
      "585/585 [==============================] - 176s - loss: 0.3908 - r2_score_k: 0.7865 - val_loss: 0.3909 - val_r2_score_k: 0.7835\n",
      "Epoch 33/300\n",
      "585/585 [==============================] - 178s - loss: 0.3782 - r2_score_k: 0.7911 - val_loss: 0.3793 - val_r2_score_k: 0.7875\n",
      "Epoch 34/300\n",
      "585/585 [==============================] - 180s - loss: 0.3666 - r2_score_k: 0.7953 - val_loss: 0.3669 - val_r2_score_k: 0.7934\n",
      "Epoch 35/300\n",
      "585/585 [==============================] - 177s - loss: 0.3564 - r2_score_k: 0.7990 - val_loss: 0.3582 - val_r2_score_k: 0.7961\n",
      "Epoch 36/300\n",
      "585/585 [==============================] - 179s - loss: 0.3468 - r2_score_k: 0.8025 - val_loss: 0.3478 - val_r2_score_k: 0.8002\n",
      "Epoch 37/300\n",
      "585/585 [==============================] - 179s - loss: 0.3380 - r2_score_k: 0.8058 - val_loss: 0.3403 - val_r2_score_k: 0.8022\n",
      "Epoch 38/300\n",
      "585/585 [==============================] - 176s - loss: 0.3300 - r2_score_k: 0.8090 - val_loss: 0.3343 - val_r2_score_k: 0.8035\n",
      "Epoch 39/300\n",
      "585/585 [==============================] - 179s - loss: 0.3226 - r2_score_k: 0.8122 - val_loss: 0.3267 - val_r2_score_k: 0.8072\n",
      "Epoch 40/300\n",
      "585/585 [==============================] - 180s - loss: 0.3160 - r2_score_k: 0.8152 - val_loss: 0.3191 - val_r2_score_k: 0.8114\n",
      "Epoch 41/300\n",
      "585/585 [==============================] - 175s - loss: 0.3099 - r2_score_k: 0.8181 - val_loss: 0.3136 - val_r2_score_k: 0.8142\n",
      "Epoch 42/300\n",
      "585/585 [==============================] - 178s - loss: 0.3043 - r2_score_k: 0.8208 - val_loss: 0.3081 - val_r2_score_k: 0.8167\n",
      "Epoch 43/300\n",
      "585/585 [==============================] - 177s - loss: 0.2994 - r2_score_k: 0.8233 - val_loss: 0.3036 - val_r2_score_k: 0.8184\n",
      "Epoch 44/300\n",
      "585/585 [==============================] - 179s - loss: 0.2947 - r2_score_k: 0.8258 - val_loss: 0.3005 - val_r2_score_k: 0.8196\n",
      "Epoch 45/300\n",
      "585/585 [==============================] - 178s - loss: 0.2908 - r2_score_k: 0.8281 - val_loss: 0.2947 - val_r2_score_k: 0.8241\n",
      "Epoch 46/300\n",
      "585/585 [==============================] - 179s - loss: 0.2873 - r2_score_k: 0.8301 - val_loss: 0.2916 - val_r2_score_k: 0.8263\n",
      "Epoch 47/300\n",
      "585/585 [==============================] - 180s - loss: 0.2842 - r2_score_k: 0.8320 - val_loss: 0.2893 - val_r2_score_k: 0.8277\n",
      "Epoch 48/300\n",
      "585/585 [==============================] - 178s - loss: 0.2815 - r2_score_k: 0.8337 - val_loss: 0.2858 - val_r2_score_k: 0.8300\n",
      "Epoch 49/300\n",
      "585/585 [==============================] - 176s - loss: 0.2793 - r2_score_k: 0.8351 - val_loss: 0.2871 - val_r2_score_k: 0.8285\n",
      "Epoch 50/300\n",
      "585/585 [==============================] - 177s - loss: 0.2770 - r2_score_k: 0.8365 - val_loss: 0.2866 - val_r2_score_k: 0.8287\n",
      "Epoch 51/300\n",
      "585/585 [==============================] - 178s - loss: 0.2751 - r2_score_k: 0.8377 - val_loss: 0.2816 - val_r2_score_k: 0.8326\n",
      "Epoch 52/300\n",
      "585/585 [==============================] - 177s - loss: 0.2733 - r2_score_k: 0.8388 - val_loss: 0.2806 - val_r2_score_k: 0.8333\n",
      "Epoch 53/300\n",
      "585/585 [==============================] - 175s - loss: 0.2716 - r2_score_k: 0.8399 - val_loss: 0.2833 - val_r2_score_k: 0.8308\n",
      "Epoch 54/300\n",
      "585/585 [==============================] - 176s - loss: 0.2702 - r2_score_k: 0.8409 - val_loss: 0.2798 - val_r2_score_k: 0.8334\n",
      "Epoch 55/300\n",
      "585/585 [==============================] - 178s - loss: 0.2687 - r2_score_k: 0.8418 - val_loss: 0.2742 - val_r2_score_k: 0.8377\n",
      "Epoch 56/300\n",
      "585/585 [==============================] - 179s - loss: 0.2674 - r2_score_k: 0.8427 - val_loss: 0.2747 - val_r2_score_k: 0.8365\n",
      "Epoch 57/300\n",
      "585/585 [==============================] - 177s - loss: 0.2662 - r2_score_k: 0.8436 - val_loss: 0.2771 - val_r2_score_k: 0.8337\n",
      "Epoch 58/300\n",
      "585/585 [==============================] - 180s - loss: 0.2649 - r2_score_k: 0.8445 - val_loss: 0.2787 - val_r2_score_k: 0.8320\n",
      "Epoch 59/300\n",
      "585/585 [==============================] - 177s - loss: 0.2638 - r2_score_k: 0.8453 - val_loss: 0.2732 - val_r2_score_k: 0.8369\n",
      "Epoch 60/300\n",
      "585/585 [==============================] - 180s - loss: 0.2628 - r2_score_k: 0.8461 - val_loss: 0.2682 - val_r2_score_k: 0.8422\n",
      "Epoch 61/300\n",
      "585/585 [==============================] - 180s - loss: 0.2618 - r2_score_k: 0.8468 - val_loss: 0.2685 - val_r2_score_k: 0.8422\n",
      "Epoch 62/300\n",
      "585/585 [==============================] - 178s - loss: 0.2608 - r2_score_k: 0.8475 - val_loss: 0.2703 - val_r2_score_k: 0.8408\n",
      "Epoch 63/300\n",
      "585/585 [==============================] - 176s - loss: 0.2599 - r2_score_k: 0.8482 - val_loss: 0.2706 - val_r2_score_k: 0.8404\n",
      "Epoch 64/300\n",
      "585/585 [==============================] - 179s - loss: 0.2590 - r2_score_k: 0.8488 - val_loss: 0.2652 - val_r2_score_k: 0.8447\n",
      "Epoch 65/300\n",
      "585/585 [==============================] - 176s - loss: 0.2581 - r2_score_k: 0.8495 - val_loss: 0.2656 - val_r2_score_k: 0.8437\n",
      "Epoch 66/300\n",
      "585/585 [==============================] - 179s - loss: 0.2574 - r2_score_k: 0.8500 - val_loss: 0.2644 - val_r2_score_k: 0.8448\n",
      "Epoch 67/300\n",
      "585/585 [==============================] - 175s - loss: 0.2566 - r2_score_k: 0.8506 - val_loss: 0.2626 - val_r2_score_k: 0.8465\n",
      "Epoch 68/300\n",
      "585/585 [==============================] - 180s - loss: 0.2558 - r2_score_k: 0.8511 - val_loss: 0.2621 - val_r2_score_k: 0.8468\n",
      "Epoch 69/300\n",
      "585/585 [==============================] - 181s - loss: 0.2551 - r2_score_k: 0.8517 - val_loss: 0.2639 - val_r2_score_k: 0.8456\n",
      "Epoch 70/300\n",
      "585/585 [==============================] - 179s - loss: 0.2543 - r2_score_k: 0.8523 - val_loss: 0.2615 - val_r2_score_k: 0.8473\n",
      "Epoch 71/300\n",
      "585/585 [==============================] - 178s - loss: 0.2536 - r2_score_k: 0.8528 - val_loss: 0.2604 - val_r2_score_k: 0.8480\n",
      "Epoch 72/300\n",
      "585/585 [==============================] - 176s - loss: 0.2530 - r2_score_k: 0.8532 - val_loss: 0.2591 - val_r2_score_k: 0.8485\n",
      "Epoch 73/300\n",
      "585/585 [==============================] - 178s - loss: 0.2524 - r2_score_k: 0.8537 - val_loss: 0.2597 - val_r2_score_k: 0.8479\n",
      "Epoch 74/300\n",
      "585/585 [==============================] - 179s - loss: 0.2517 - r2_score_k: 0.8542 - val_loss: 0.2588 - val_r2_score_k: 0.8485\n",
      "Epoch 75/300\n",
      "585/585 [==============================] - 178s - loss: 0.2512 - r2_score_k: 0.8546 - val_loss: 0.2573 - val_r2_score_k: 0.8500\n",
      "Epoch 76/300\n",
      "585/585 [==============================] - 178s - loss: 0.2506 - r2_score_k: 0.8550 - val_loss: 0.2596 - val_r2_score_k: 0.8482\n",
      "Epoch 77/300\n",
      "585/585 [==============================] - 179s - loss: 0.2499 - r2_score_k: 0.8555 - val_loss: 0.2626 - val_r2_score_k: 0.8458\n",
      "Epoch 78/300\n",
      "585/585 [==============================] - 180s - loss: 0.2493 - r2_score_k: 0.8560 - val_loss: 0.2586 - val_r2_score_k: 0.8488\n",
      "Epoch 79/300\n",
      "585/585 [==============================] - 178s - loss: 0.2487 - r2_score_k: 0.8564 - val_loss: 0.2555 - val_r2_score_k: 0.8510\n",
      "Epoch 80/300\n",
      "585/585 [==============================] - 178s - loss: 0.2482 - r2_score_k: 0.8567 - val_loss: 0.2573 - val_r2_score_k: 0.8492\n",
      "Epoch 81/300\n",
      "585/585 [==============================] - 177s - loss: 0.2478 - r2_score_k: 0.8571 - val_loss: 0.2594 - val_r2_score_k: 0.8472\n",
      "Epoch 82/300\n",
      "585/585 [==============================] - 177s - loss: 0.2472 - r2_score_k: 0.8575 - val_loss: 0.2549 - val_r2_score_k: 0.8513\n",
      "Epoch 83/300\n",
      "585/585 [==============================] - 178s - loss: 0.2468 - r2_score_k: 0.8578 - val_loss: 0.2538 - val_r2_score_k: 0.8521\n",
      "Epoch 84/300\n",
      "585/585 [==============================] - 179s - loss: 0.2463 - r2_score_k: 0.8582 - val_loss: 0.2557 - val_r2_score_k: 0.8508\n",
      "Epoch 85/300\n",
      "585/585 [==============================] - 178s - loss: 0.2457 - r2_score_k: 0.8586 - val_loss: 0.2549 - val_r2_score_k: 0.8514\n",
      "Epoch 86/300\n",
      "585/585 [==============================] - 179s - loss: 0.2453 - r2_score_k: 0.8589 - val_loss: 0.2533 - val_r2_score_k: 0.8526\n",
      "Epoch 87/300\n",
      "585/585 [==============================] - 180s - loss: 0.2449 - r2_score_k: 0.8592 - val_loss: 0.2523 - val_r2_score_k: 0.8532\n",
      "Epoch 88/300\n",
      "585/585 [==============================] - 180s - loss: 0.2444 - r2_score_k: 0.8595 - val_loss: 0.2532 - val_r2_score_k: 0.8521\n",
      "Epoch 89/300\n",
      "585/585 [==============================] - 177s - loss: 0.2440 - r2_score_k: 0.8599 - val_loss: 0.2550 - val_r2_score_k: 0.8503\n",
      "Epoch 90/300\n",
      "585/585 [==============================] - 180s - loss: 0.2436 - r2_score_k: 0.8601 - val_loss: 0.2536 - val_r2_score_k: 0.8514\n",
      "Epoch 91/300\n",
      "585/585 [==============================] - 180s - loss: 0.2431 - r2_score_k: 0.8604 - val_loss: 0.2509 - val_r2_score_k: 0.8541\n",
      "Epoch 92/300\n",
      "585/585 [==============================] - 177s - loss: 0.2427 - r2_score_k: 0.8608 - val_loss: 0.2514 - val_r2_score_k: 0.8541\n",
      "Epoch 93/300\n",
      "585/585 [==============================] - 176s - loss: 0.2423 - r2_score_k: 0.8610 - val_loss: 0.2522 - val_r2_score_k: 0.8535\n",
      "Epoch 94/300\n",
      "585/585 [==============================] - 178s - loss: 0.2419 - r2_score_k: 0.8614 - val_loss: 0.2514 - val_r2_score_k: 0.8542\n",
      "Epoch 95/300\n",
      "585/585 [==============================] - 177s - loss: 0.2415 - r2_score_k: 0.8616 - val_loss: 0.2500 - val_r2_score_k: 0.8552\n",
      "Epoch 96/300\n",
      "585/585 [==============================] - 181s - loss: 0.2411 - r2_score_k: 0.8619 - val_loss: 0.2498 - val_r2_score_k: 0.8556\n",
      "Epoch 97/300\n",
      "585/585 [==============================] - 179s - loss: 0.2408 - r2_score_k: 0.8622 - val_loss: 0.2490 - val_r2_score_k: 0.8557\n",
      "Epoch 98/300\n",
      "585/585 [==============================] - 179s - loss: 0.2404 - r2_score_k: 0.8625 - val_loss: 0.2517 - val_r2_score_k: 0.8530\n",
      "Epoch 99/300\n",
      "585/585 [==============================] - 180s - loss: 0.2400 - r2_score_k: 0.8627 - val_loss: 0.2513 - val_r2_score_k: 0.8535\n",
      "Epoch 100/300\n",
      "585/585 [==============================] - 177s - loss: 0.2397 - r2_score_k: 0.8629 - val_loss: 0.2483 - val_r2_score_k: 0.8565\n",
      "Epoch 101/300\n",
      "585/585 [==============================] - 177s - loss: 0.2394 - r2_score_k: 0.8632 - val_loss: 0.2499 - val_r2_score_k: 0.8558\n",
      "Epoch 102/300\n",
      "585/585 [==============================] - 178s - loss: 0.2390 - r2_score_k: 0.8635 - val_loss: 0.2491 - val_r2_score_k: 0.8564\n",
      "Epoch 103/300\n",
      "585/585 [==============================] - 181s - loss: 0.2387 - r2_score_k: 0.8636 - val_loss: 0.2476 - val_r2_score_k: 0.8575\n",
      "Epoch 104/300\n",
      "585/585 [==============================] - 178s - loss: 0.2383 - r2_score_k: 0.8639 - val_loss: 0.2479 - val_r2_score_k: 0.8569\n",
      "Epoch 105/300\n",
      "585/585 [==============================] - 177s - loss: 0.2380 - r2_score_k: 0.8641 - val_loss: 0.2519 - val_r2_score_k: 0.8532\n",
      "Epoch 106/300\n",
      "585/585 [==============================] - 179s - loss: 0.2377 - r2_score_k: 0.8644 - val_loss: 0.2499 - val_r2_score_k: 0.8552\n",
      "Epoch 107/300\n",
      "585/585 [==============================] - 179s - loss: 0.2374 - r2_score_k: 0.8646 - val_loss: 0.2465 - val_r2_score_k: 0.8583\n",
      "Epoch 108/300\n",
      "585/585 [==============================] - 179s - loss: 0.2370 - r2_score_k: 0.8648 - val_loss: 0.2516 - val_r2_score_k: 0.8542\n",
      "Epoch 109/300\n",
      "585/585 [==============================] - 179s - loss: 0.2367 - r2_score_k: 0.8650 - val_loss: 0.2536 - val_r2_score_k: 0.8525\n",
      "Epoch 110/300\n",
      "585/585 [==============================] - 177s - loss: 0.2365 - r2_score_k: 0.8652 - val_loss: 0.2482 - val_r2_score_k: 0.8569\n",
      "Epoch 111/300\n",
      "585/585 [==============================] - 179s - loss: 0.2361 - r2_score_k: 0.8654 - val_loss: 0.2456 - val_r2_score_k: 0.8583\n",
      "Epoch 112/300\n",
      "585/585 [==============================] - 178s - loss: 0.2359 - r2_score_k: 0.8656 - val_loss: 0.2512 - val_r2_score_k: 0.8528\n",
      "Epoch 113/300\n",
      "585/585 [==============================] - 178s - loss: 0.2356 - r2_score_k: 0.8659 - val_loss: 0.2530 - val_r2_score_k: 0.8507\n",
      "Epoch 114/300\n",
      "585/585 [==============================] - 177s - loss: 0.2353 - r2_score_k: 0.8660 - val_loss: 0.2461 - val_r2_score_k: 0.8571\n",
      "Epoch 115/300\n",
      "585/585 [==============================] - 179s - loss: 0.2351 - r2_score_k: 0.8662 - val_loss: 0.2477 - val_r2_score_k: 0.8567\n"
     ]
    }
   ],
   "source": [
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD1 0.752808820729\n",
      "SD2 0.751343600989\n",
      "SDCrypt 0.582268528334\n",
      "SDNew 0.819467765646\n"
     ]
    }
   ],
   "source": [
    "eval_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save model to file to retreive later\n",
    "model.save(resultsdir + \"model_seq_57_full.h5\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:gpu-tensorflow]",
   "language": "python",
   "name": "conda-env-gpu-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
